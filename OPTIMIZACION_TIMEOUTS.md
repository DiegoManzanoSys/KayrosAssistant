# ‚ö° Optimizaci√≥n de Timeouts para Ollama

## üîç Problema Identificado

```
‚ö†Ô∏è Error: timeout of 60000ms exceeded
```

Este error ocurre porque **Ollama ejecuta el modelo LLM localmente**, lo cual puede tardar m√°s de 60 segundos dependiendo de:
- Tama√±o del documento
- Hardware disponible (CPU vs GPU)
- Complejidad del an√°lisis
- Carga del sistema

---

## ‚úÖ Soluciones Implementadas

### 1. **Frontend - Timeout Extendido**

**Archivo:** `frontresume/.env.local`
```env
# Antes
NEXT_PUBLIC_API_TIMEOUT=60000  # 1 minuto

# Ahora
NEXT_PUBLIC_API_TIMEOUT=300000 # 5 minutos
```

**Archivo:** `frontresume/src/lib/api/endpoints.ts`
```typescript
export const summarizeDocument = async (
  request: SummarizeRequest
): Promise<SummarizeResponse> => {
  // ...
  const response = await apiClient.post<SummarizeResponse>('/api/summarize', formData, {
    headers: {
      'Content-Type': 'multipart/form-data',
    },
    timeout: 300000, // ‚≠ê 5 minutos para documentos largos con Ollama
  });
  return response.data;
};
```

### 2. **Manejo de Errores de Timeout Mejorado**

**Archivo:** `frontresume/src/lib/api/client.ts`
```typescript
// Mensajes espec√≠ficos para timeouts
case 504:
  apiError.message = 'Tiempo de espera agotado. El documento puede ser muy grande...';
  break;

// Detecci√≥n de timeout de Axios
if (error.code === 'ECONNABORTED' || error.message.includes('timeout')) {
  apiError.message = 'Tiempo de espera agotado. Ollama est√° tardando mucho...';
}
```

### 3. **Instalaci√≥n de Librer√≠a Ollama**

```bash
cd project
venv\Scripts\activate
pip install ollama
```

‚úÖ **Ollama 0.6.1** instalado correctamente

---

## üöÄ C√≥mo Aplicar los Cambios

### Opci√≥n 1: Reiniciar con start-all.bat
```bash
# Detener servicios actuales
stop-all.bat

# Reiniciar todo
start-all.bat
```

### Opci√≥n 2: Reinicio Manual

**Frontend:**
```bash
cd frontresume
# Detener (Ctrl+C en la terminal)
npm run dev
```

**Backend:**
```bash
cd project
venv\Scripts\activate
uvicorn app.main:app --reload
```

---

## ‚è±Ô∏è Tiempos Estimados de Procesamiento

| Tama√±o del Documento | Hardware | Tiempo Estimado |
|----------------------|----------|-----------------|
| 1-2 p√°ginas (PDF) | CPU | 10-30 segundos |
| 1-2 p√°ginas (PDF) | GPU | 5-15 segundos |
| 5-10 p√°ginas (PDF) | CPU | 30-90 segundos |
| 5-10 p√°ginas (PDF) | GPU | 15-45 segundos |
| 20+ p√°ginas (PDF) | CPU | 2-5 minutos |
| 20+ p√°ginas (PDF) | GPU | 1-2 minutos |

> **Nota:** Estos tiempos son aproximados y dependen de tu hardware espec√≠fico.

---

## üéØ Recomendaciones de Uso

### 1. **Documentos Grandes**
Si tienes documentos muy grandes (>20 p√°ginas):
- Usa el tipo de resumen `tldr` o `bullets` (m√°s r√°pido)
- Reduce `max_tokens` a 512 o menos
- Considera dividir el documento

### 2. **Acelerar Ollama con GPU**

Si tienes una GPU NVIDIA:
```bash
# Verificar que Ollama use GPU
ollama list

# Deber√≠a mostrar: "GPU: NVIDIA ..."
```

Si no detecta GPU:
1. Instala NVIDIA CUDA Toolkit
2. Reinicia Ollama: `ollama serve`

### 3. **Ajustar Par√°metros del Modelo**

**Archivo:** `project/app/services/ai_client.py`
```python
# Para respuestas m√°s r√°pidas
response = client.chat(
    model=OLLAMA_MODEL,
    messages=[...],
    options={
        "num_predict": 512,      # ‚¨áÔ∏è Reduce tokens = m√°s r√°pido
        "temperature": 0.2,
        "num_ctx": 2048,         # ‚¨áÔ∏è Contexto m√°s peque√±o
    }
)
```

### 4. **Modelos Alternativos M√°s R√°pidos**

Si `llama3.1:latest` es muy lento, prueba modelos m√°s peque√±os:

```bash
# Llama 3.2 (m√°s r√°pido, menor tama√±o)
ollama pull llama3.2:latest

# Mistral (alternativa r√°pida)
ollama pull mistral:latest

# Cambiar en .env
OLLAMA_MODEL=llama3.2:latest
```

---

## üìä Configuraci√≥n Actual

| Par√°metro | Valor | Descripci√≥n |
|-----------|-------|-------------|
| **API Timeout Global** | 300,000 ms | 5 minutos |
| **Summarize Timeout** | 300,000 ms | 5 minutos |
| **Chunk Size** | 2,500 chars | Tama√±o de divisi√≥n de texto |
| **Max Tokens** | 1,024 | Tokens por chunk |
| **Temperature** | 0.2 | Consistencia del modelo |

---

## üêõ Troubleshooting

### Error Persiste Despu√©s de 5 Minutos

**Problema:** Documento demasiado grande o hardware muy lento.

**Soluci√≥n:**
1. Reduce el tama√±o del documento
2. Usa `summary_type: "tldr"` para res√∫menes m√°s cortos
3. Reduce `max_tokens` a 256-512
4. Considera usar un modelo m√°s peque√±o

### Ollama No Responde

**Problema:** Ollama no est√° corriendo o se qued√≥ colgado.

**Soluci√≥n:**
```bash
# Windows
taskkill /IM ollama.exe /F
ollama serve

# Verificar
ollama list
```

### Backend Se Queda Procesando

**Problema:** El proceso de Uvicorn se qued√≥ esperando respuesta de Ollama.

**Soluci√≥n:**
```bash
# Reiniciar backend
cd project
venv\Scripts\activate
uvicorn app.main:app --reload
```

### Frontend Muestra "Loading" Infinitamente

**Problema:** El timeout se alcanz√≥ pero el UI no se actualiz√≥.

**Soluci√≥n:**
```bash
# Limpiar cach√© de Next.js
cd frontresume
Remove-Item -Path .next -Recurse -Force
npm run dev
```

---

## üìà Monitoreo de Rendimiento

### Ver Logs de Ollama
```bash
# Windows (buscar proceso)
Get-Process ollama

# Ver logs en tiempo real
ollama serve
```

### Ver Logs del Backend
Los logs de FastAPI muestran el tiempo de procesamiento:
```
INFO:     127.0.0.1:XXXX - "POST /api/summarize HTTP/1.1" 200 OK
```

### Medir Tiempo en el Frontend
Abre DevTools ‚Üí Network ‚Üí Busca `/api/summarize` ‚Üí Ver "Time"

---

## üîß Configuraci√≥n Avanzada

### Aumentar Timeout Solo para Endpoints Espec√≠ficos

Si otros endpoints son m√°s r√°pidos, puedes mantener timeouts diferentes:

```typescript
// endpoints.ts

// Resumen: 5 minutos
export const summarizeDocument = async (...) => {
  const response = await apiClient.post(..., {
    timeout: 300000, // 5 min
  });
};

// Keywords: 2 minutos (m√°s r√°pido)
export const extractKeywords = async (...) => {
  const response = await apiClient.post(..., {
    timeout: 120000, // 2 min
  });
};
```

### Timeout Din√°mico Seg√∫n Tama√±o del Archivo

```typescript
export const summarizeDocument = async (
  request: SummarizeRequest
): Promise<SummarizeResponse> => {
  const fileSizeMB = request.file.size / (1024 * 1024);
  
  // 1 minuto por cada MB
  const dynamicTimeout = Math.max(60000, fileSizeMB * 60000);
  
  const response = await apiClient.post(..., {
    timeout: dynamicTimeout,
  });
};
```

---

## ‚úÖ Checklist de Verificaci√≥n

Despu√©s de aplicar cambios:

- [ ] Frontend reiniciado con nuevo `.env.local`
- [ ] Backend reiniciado con `ollama` instalado
- [ ] Ollama corriendo en http://localhost:11434
- [ ] Modelo `llama3.1:latest` descargado
- [ ] Probado con un documento peque√±o (1-2 p√°ginas)
- [ ] Probado con un documento mediano (5-10 p√°ginas)
- [ ] Errores de timeout ya no aparecen

---

## üìö Referencias

- [Documentaci√≥n de Ollama](https://ollama.com/docs)
- [Axios Timeout Configuration](https://axios-http.com/docs/req_config)
- [FastAPI Timeouts](https://fastapi.tiangolo.com/)

---

**√öltima actualizaci√≥n:** 8 de Diciembre, 2025

**Estado:** ‚úÖ Timeouts extendidos a 5 minutos
