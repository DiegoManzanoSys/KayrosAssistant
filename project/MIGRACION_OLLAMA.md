# üîÑ Migraci√≥n a Ollama - Modelo Local

## üìù Cambios Realizados

El backend ha sido refactorizado para usar **Ollama** con el modelo **llama3.1:latest** en lugar de Groq API.

### ‚úÖ Ventajas de Ollama

- üöÄ **100% Local** - No necesitas API keys ni conexi√≥n a internet
- üí∞ **Gratuito** - Sin costos de API
- üîí **Privado** - Tus datos no salen de tu m√°quina
- ‚ö° **R√°pido** - Procesamiento local sin latencia de red

## üì¶ Requisitos Previos

### 1. Instalar Ollama

**Windows:**
```powershell
# Descargar desde: https://ollama.com/download
# O usar winget:
winget install Ollama.Ollama
```

**Linux:**
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

**macOS:**
```bash
brew install ollama
```

### 2. Descargar el Modelo llama3.1

```bash
ollama pull llama3.1:latest
```

### 3. Verificar que Ollama est√° corriendo

```bash
ollama list
```

Deber√≠as ver:
```
NAME               ID              SIZE      MODIFIED
llama3.1:latest    46e0c10c039e    4.9 GB    X seconds ago
```

## ‚öôÔ∏è Configuraci√≥n

### Archivo `.env`

El archivo `.env` ya no requiere `GROQ_API_KEY`. Ahora usa:

```env
# Configuraci√≥n de Ollama
OLLAMA_MODEL=llama3.1:latest
OLLAMA_BASE_URL=http://localhost:11434
TMP_DIR=/tmp/resumeai
```

### Variables de Entorno

| Variable | Descripci√≥n | Valor por Defecto |
|----------|-------------|-------------------|
| `OLLAMA_MODEL` | Modelo de Ollama a usar | `llama3.1:latest` |
| `OLLAMA_BASE_URL` | URL del servidor Ollama | `http://localhost:11434` |

## üîß Cambios T√©cnicos

### Archivos Modificados

1. **`requirements.txt`**
   - ‚ùå Eliminado: `requests` (para Groq)
   - ‚úÖ Agregado: `ollama>=0.1.0`

2. **`app/services/ai_client.py`**
   - ‚ùå Eliminado: `call_groq_api()`
   - ‚úÖ Agregado: `call_ollama_api()`
   - ‚ùå Eliminado: `summarize_text_with_groq()`
   - ‚úÖ Agregado: `summarize_text_with_ollama()`

3. **`app/api/summarize.py`**
   - Actualizado para usar `summarize_text_with_ollama()`

4. **`app/api/analysis.py`**
   - Actualizado para usar `call_ollama_api()`

### Funci√≥n Principal

```python
def call_ollama_api(prompt: str, max_tokens: int = 1024) -> str:
    """
    Llamada a Ollama usando el modelo local llama3.1.
    """
    try:
        client = ollama.Client(host=OLLAMA_BASE_URL)
        
        response = client.chat(
            model=OLLAMA_MODEL,
            messages=[
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            options={
                "num_predict": max_tokens,
                "temperature": 0.2,
            }
        )
        
        return response['message']['content']
        
    except Exception as e:
        raise RuntimeError(f"Ollama API error: {str(e)}")
```

## üöÄ C√≥mo Usar

### Paso 1: Asegurar que Ollama est√© corriendo

```bash
# En una terminal, iniciar Ollama (si no est√° corriendo)
ollama serve
```

### Paso 2: Instalar dependencias actualizadas

```bash
cd project
venv\Scripts\activate
pip install -r requirements.txt
```

### Paso 3: Iniciar el backend

```bash
# Opci√≥n 1: Usar start.bat
start.bat

# Opci√≥n 2: Manual
venv\Scripts\activate
uvicorn app.main:app --reload
```

### Paso 4: Usar desde el frontend

El frontend ya est√° configurado para conectarse al backend en `http://localhost:8000`. No requiere cambios.

## üéØ Rendimiento

### Comparaci√≥n Groq vs Ollama

| M√©trica | Groq (Nube) | Ollama (Local) |
|---------|-------------|----------------|
| Latencia | ~1-3s | ~2-5s* |
| Costo | $$ | Gratis |
| Privacidad | ‚ùå | ‚úÖ |
| Requiere Internet | ‚úÖ | ‚ùå |
| L√≠mites de API | ‚úÖ | ‚ùå |

*Depende de tu hardware (GPU/CPU)

### Optimizaci√≥n

Para mejor rendimiento con Ollama:

1. **Usar GPU** (si est√° disponible):
   ```bash
   # Ollama detecta autom√°ticamente CUDA/ROCm
   ollama list
   ```

2. **Ajustar par√°metros**:
   ```python
   options={
       "num_predict": 512,  # Menos tokens = m√°s r√°pido
       "temperature": 0.2,   # Menor temperatura = m√°s determinista
   }
   ```

## üîÑ Otros Modelos Disponibles

Puedes cambiar el modelo en `.env`:

```bash
# Modelos disponibles en Ollama
ollama list

# Ejemplos de modelos alternativos:
OLLAMA_MODEL=llama3.2:latest     # M√°s r√°pido, menor tama√±o
OLLAMA_MODEL=mistral:latest      # Alternativa a llama
OLLAMA_MODEL=codellama:latest    # Especializado en c√≥digo
```

Descargar nuevos modelos:
```bash
ollama pull llama3.2:latest
ollama pull mistral:latest
```

## üêõ Soluci√≥n de Problemas

### Error: "Ollama API error: ..."

**Problema**: Ollama no est√° corriendo.

**Soluci√≥n**:
```bash
ollama serve
```

### Error: "Model not found"

**Problema**: El modelo no est√° descargado.

**Soluci√≥n**:
```bash
ollama pull llama3.1:latest
```

### Respuestas lentas

**Problema**: CPU no optimizado.

**Soluci√≥n**:
- Usar un modelo m√°s peque√±o: `ollama pull llama3.2:latest`
- Verificar que Ollama use GPU si est√° disponible

### Puerto 11434 en uso

**Problema**: Ollama ya est√° corriendo o el puerto est√° ocupado.

**Soluci√≥n**:
```bash
# Windows
netstat -ano | findstr :11434
taskkill /PID <PID> /F

# Linux/Mac
lsof -ti:11434 | xargs kill -9
```

## üìö Recursos

- [Documentaci√≥n de Ollama](https://ollama.com/docs)
- [Modelos Disponibles](https://ollama.com/library)
- [Python SDK de Ollama](https://github.com/ollama/ollama-python)

## ‚úÖ Checklist de Migraci√≥n

- [x] Instalar Ollama
- [x] Descargar modelo llama3.1:latest
- [x] Actualizar `.env` (eliminar GROQ_API_KEY)
- [x] Instalar nuevas dependencias (`pip install -r requirements.txt`)
- [x] Verificar que Ollama est√© corriendo
- [x] Iniciar backend
- [x] Probar endpoints desde frontend

## üéâ ¬°Listo!

Ahora tu aplicaci√≥n funciona 100% localmente sin depender de APIs externas.
